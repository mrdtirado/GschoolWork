{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The goal of any model building process is to increase the predictive performance of a statistical model. Today you'll learn the difference between a training set, test set, validation set and a process known as cross-validation - and ultimately how these varying methods have a direct effect on your models predictive performance on unseen data.\n",
    "\n",
    "Since we normally only have access to a fixed data set, it's common to split the original data set into two portions named a **train** and **test** set - of which we'll use the test portion to simulate 'unseen data.'\n",
    "\n",
    "How we deal with the **train** portion of the original data set will be the focus of this mornings sprint. Below are four different ways to build upon the complexity of our training strategy to produce the best process for trianing and validating our models.\n",
    "\n",
    "- **Worst Option** - Train model with original data set without spliting into train and test set. Unable to score our model and determine its predictive performance since we don't have test set.\n",
    "- **Bad Option** - Only perform a train-test split. Train model with entire training set and score against test set.\n",
    "- **Better Option** - Further split training set into **one** smaller training set and **one** validation set. Use validation set score to guide our model choice and then score best model against test set. *Below is an image of this option.*\n",
    "- **Even Better Option** - Cross validate training set by spliting into **many** training sets and **many** validation sets. Rotate through each portion to build and validate model and average results. Best model is used to score against test set.\n",
    "\n",
    "- **Training Set** - Used to train one, or more, models.\n",
    "- **Validation Set** - Used to tune hyperparameters of different models and choose the best performing model.\n",
    "- **Test Set** - Used to test the predictive performance of the best scoring model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2: Train and Test Split Only (Bad Option)\n",
    "\n",
    "The reason this option is considered a poor chioce is two-fold: 1) High Variance - The split into a train and test set could randomly be such that the training set is not representative of the test set. This could mean our estimate of the performance of our model is wrong because the test set is unlike other data we will see. 2) No validation set for model tuning -- If we want to iterate on our model (for hyper-parameter optimization, or variable selection) on the basis of its test-set performance we can use the test set for this purpose, but without a validation set, we can't get an estimate of how our model will perform on truly unseen data, because the model has been able to \"see\" the test set. As a result estimates of the performance of the model are likely to be _optimistic_ when calculated based on training data for which the model has been selected to perform as well as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Since we already split the original data set on Part 1, we'll train our model on the training set only. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a function `rmse(true, predicted)` that takes your `true` and `predicted` values and calculates\n",
    "   the RMSE. You should use `sklearn.metrics.mean_squared_error()` to confirm your\n",
    "   results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(targets, predictions):\n",
    "    return np.sqrt(((targets - predictions) ** 2).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use `LinearRegression()` in scikit-learn to build a model with your training data.\n",
    "\n",
    "   Note that there is multicollinearity and other issues in the data.  Do not worry\n",
    "   about this for now. We will learn about Lasso and Ridge regularization this\n",
    "   afternoon (alternative to the methods you have learned yesterday) to\n",
    "   deal with some of the issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "X = boston.data # housing features\n",
    "y = boston.target # housing prices\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 13) (127, 13)\n",
      "(379,) (127,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training set  4.77717611421783\n",
      "RMSE for test set  4.77717611421783\n"
     ]
    }
   ],
   "source": [
    "# Fit your model using the training set\n",
    "linear = LinearRegression()\n",
    "linear.fit(X_train, y_train)\n",
    "\n",
    "# Call predict to get the predicted values for training and test set\n",
    "train_predicted = linear.predict(X_train)\n",
    "test_predicted = linear.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate RMSE for training and test set\n",
    "print( 'RMSE for training set ', rmse(y_test, test_predicted) )\n",
    "print( 'RMSE for test set ', rmse(y_test, test_predicted) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.77717611421783"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(sklearn.metrics.mean_squared_error(y_test, test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Which RMSE did you expect to be higher?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I expected the error to be the same for both training and test set because they are from the same population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Explain the value of evaluating RMSE on a separate test set (instead of fitting a\n",
    "   model and calculating RMSE on the entire data set).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to see if our validation test set is fundamentally different than our training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: K-fold Cross Validation (Even Better Option)\n",
    "\n",
    "In K-fold cross validation, we'll split our training set into **k** groups, usually 5 or 10. One of the k groups will act as our validation set, the rest of the (**k-1**) groups will be the training set. We'll iterate through each combination until each **fold** has had a chance to act as our validation set. At each iteration, a metric for accuracy (RMSE in this case) will be calculated and an average score will be calculated across the k iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
