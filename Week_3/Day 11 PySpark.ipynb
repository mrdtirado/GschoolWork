{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "import json\n",
    "\n",
    "spark = (ps.sql.SparkSession.builder \n",
    "        .master(\"local[4]\") \n",
    "         \n",
    "         \n",
    "        .appName(\"morning sprint\") \n",
    "        .getOrCreate()\n",
    "        )\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark operates in **[Resilient Distributed Datasets](http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds) (RDDs). An RDD is\n",
    "a collection of data partitioned across machines**. RDDs allow the processing\n",
    "of data to be parallelized due to the partitions. RDDs can be created from\n",
    "a SparkContext in two ways: loading an external dataset, or by parallelizing\n",
    "an existing collection of objects in your currently running program (in our\n",
    "Python programs, this is often times a list).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create an RDD from a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_rdd = sc.parallelize([1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Read an RDD in from a text file. **By default, the RDD will treat each line\n",
    "as an item and read it in as string.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_link = '/home/asus/DSI_Lectures/spark/natalie_hunt/'\n",
    "file_rdd = sc.textFile(dir_link + 'data/cookie_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"Jane\": \"2\"}', '{\"Jane\": \"1\"}']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_rdd.first() # Returns the first entry in the RDD\n",
    "file_rdd.take(2) # Returns the first two entries in the RDD as a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To retrieve all the items in your RDD, every partition in the RDD has to be\n",
    "  accessed, and this could take a long time. In general, before you execute\n",
    "  commands (like the following) to retrieve all the items in your RDD, you\n",
    "  should be aware of how many entries you are pulling. Keep in mind that to\n",
    "  execute the `.collect()` method on the RDD object (like we do below), your entire\n",
    "  dataset must fit in memory in your driver program (we in general don't want\n",
    "  to call `.collect()` on very large datasets).\n",
    "\n",
    "  The standard workflow when working with RDDs is to perform all the big data\n",
    "  operations/transformations **before** you pool/retrieve the results. If the\n",
    "  results can't be collected onto your driver program, it's common to write\n",
    "  data out to a distributed storage system, like HDFS or S3.\n",
    "\n",
    "  With that said, we can retrieve all the items from our RDD as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"Jane\": \"2\"}',\n",
       " '{\"Jane\": \"1\"}',\n",
       " '{\"Pete\": \"20\"}',\n",
       " '{\"Tyler\": \"3\"}',\n",
       " '{\"Duncan\": \"4\"}',\n",
       " '{\"Yuki\": \"5\"}',\n",
       " '{\"Duncan\": \"6\"}',\n",
       " '{\"Duncan\": \"4\"}',\n",
       " '{\"Duncan\": \"5\"}']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Intro to Functional Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark operations fit within the [functional programming paradigm](https://en.wikipedia.org/wiki/Functional_programming).\n",
    "In terms of our RDD objects, this means that our RDD objects are immutable and that\n",
    "anytime we apply a **transformation** to an RDD (such as `.map()`, `.reduceByKey()`,\n",
    "or `.filter()`) it returns another RDD.\n",
    "\n",
    "Transformations in Spark are lazy, this means that performing a transformation does\n",
    "not cause computations to be performed. Instead, an RDD remembers the chain of\n",
    "transformations that you define and computes them all only when and action requires\n",
    "a result to be returned.\n",
    "\n",
    "**Spark notes**:\n",
    "\n",
    "  * A lot of Spark's functionalities assume the items in an RDD to be tuples\n",
    "  of `(key, value)` pairs, so often times it can be useful to structure your\n",
    "  RDDs this way.\n",
    "  * Beware of [lazy evaluation](https://en.wikipedia.org/wiki/Lazy_evaluation), where transformations\n",
    "  on the RDD are not executed until an **action** is executed on the RDD\n",
    "  to retrieve items from it (such as `.collect()`, `.first()`, `.take()`, or\n",
    "  `.count()`). So if you are doing a lot transformations in a row, it can\n",
    "  be helpful to call `.first()` in between to ensure your transformations are\n",
    "  running properly.\n",
    "  * If you are not sure what RDD transformations/actions there are, you can\n",
    "  check out the [docs](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn the items in `file_rdd` into `(key, value)` pairs using `.map()`. In order to do that, you'll find a template function `parse_json_first_key_pair` in the `spark_intro.py` file. Implement this function that takes a json formatted string (use `json.loads()`) and output the key,value pair you need. Test it with the string `u'{\"Jane\": \"2\"}'`, your function should return `(u'Jane', 2)`. **Remember to cast value as type** `int`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_first_key_pair(json_string):\n",
    "    json_data = json.loads(json_string)\n",
    "    for k,v in json_data.items():\n",
    "        return(k,int(v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Jane', 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_json_first_key_pair('{\"Jane\": \"2\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 99\n",
    "y = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Yuki', 5), ('Duncan', 6), ('Pete', 20)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_rdd = sc.textFile(dir_link + 'data/cookie_data.txt')\\\n",
    "            .map(lambda x: parse_json_first_key_pair(x))\\\n",
    "            .filter(lambda x: (x[1] >= 5))\\\n",
    "            .reduceByKey(lambda x, y: max(x,y))\\\n",
    "            .sortBy(lambda x: x[1])\n",
    "file_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Yuki', 5), ('Duncan', 6)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
